#!/usr/bin/env python3
"""data_split.py

Create 5 disjoint data splits following the protocol described in the paper:

•   **Risk‑model data (χ^Risk)**
    – IEMOCAP **2 sessions**
    – CREMA‑D   **40 % actors**
    – RAVDESS   **40 % actors**
    Each corpus‑specific subset is then split 75 / 25 into train/val.

•   **Transform‑model data (χ^R)**
    – IEMOCAP **2 different sessions** (non‑overlapping with Risk)
    – CREMA‑D   **another 40 % actors** (disjoint)
    – RAVDESS   **another 40 % actors** (disjoint)
    Again 75 / 25 train/val.

•   **Inference / Test data (χ^I)**
    – All remaining utterances not in the above two training sets.

Five full data sets are generated by *rotating* the session/actor splits so
that every item appears in the test set exactly once and **no test item
appears in more than one fold**.


**Output directory structure** (for --out_dir DIR):

```
DIR/
  fold_0/
    risk_train.csv  risk_val.csv
    trans_train.csv trans_val.csv
    test.csv
  fold_1/ ...
  ...
  fold_4/ ...
```

Each CSV has columns: ``wav_file,gender,emotion,corpus`` and optional
``actor_id,session`` for easier debugging.

Example usage
-------------

```bash
python data_split.py \
    --iemocap_csv /datasets/IEMOCAP/df_iemocap_clean.csv \
    --cremad_csv   /datasets/CREMA-D/df_cremad_clean.csv \
    --ravdess_csv  /datasets/RAVDESS/ravdess_4emo.csv \
    --out_dir      ./splits --seed 42
```

A short summary for every fold is printed to stdout so you can sanity‑check
counts & balance.
"""

import argparse
import os
import random
from pathlib import Path
from typing import List, Tuple
import re

import numpy as np
import pandas as pd


# ---------------------------------------------------------------------------
# Helper functions
# ---------------------------------------------------------------------------


def balanced_partition(labels, k, rng):
    """
    Partition an iterable of *labels* into k ~equal disjoint lists (order shuffled).
    Returns list of k lists.
    """
    items = list(labels)
    rng.shuffle(items)
    return [list(chunk) for chunk in np.array_split(items, k)]
        

def create_fold_roles(groups, fold):
    """
    Given 5 groups [A,B,C,D,E], return (test_ids, risk_ids, trans_ids) for fold.
    Pattern:
        f=0: test=A, risk=B+C, trans=D+E
        f=1: test=B, risk=C+D, trans=E+A
        ...
    """
    assert len(groups) == 5, "Expected exactly 5 groups"
    A, B, C, D, E = groups
    if fold == 0:
        return A, B + C, D + E
    elif fold == 1:
        return B, C + D, E + A
    elif fold == 2: 
        return C, D + E, A + B
    elif fold == 3:
        return D, E + A, B + C
    elif fold == 4:
        return E, A + B, C + D
    raise ValueError(f"Invalid fold {fold}, expected 0-4")


def _parse_iemocap_session(wav_file: str) -> int:
    """Extract session index 0‑4 from a wav_file like 'Ses01F_impro01_F000'."""
    return int(wav_file[3:5]) - 1  # 0‑based (01→0 … 05→4)


# def _select_actor_ids(df: pd.DataFrame, frac: float, exclude: set, rng: random.Random) -> List[int]:
#     """Randomly pick *frac* proportion of unique actor_ids from df, avoiding *exclude*."""
#     actors = sorted(set(df["actor_id"]) - exclude)
#     k = int(round(len(actors) * frac))
#     rng.shuffle(actors)
#     return actors[:k]


def _train_val_split(df: pd.DataFrame, train_ratio: float, rng: np.random.Generator) -> Tuple[pd.DataFrame, pd.DataFrame]:
    msk = rng.random(len(df)) < train_ratio
    return df[msk].reset_index(drop=True), df[~msk].reset_index(drop=True)


# ----------------------------------------------------------------------------
# ADD actor_id column to CSVs
# ----------------------------------------------------------------------------
def ensure_actor_id(df: pd.DataFrame, dataset: str) -> pd.DataFrame:
    if 'actor_id' in df.columns:
        return df
    
    # -------- IEMOCAP --------
    if dataset.lower() == 'iemocap':
        df['actor_id'] = df['wav_file'].apply(lambda x: re.search(r'Ses(\d{2})', x).group(1))
        df['actor_id'] = df['actor_id'].astype(int)
        return df

    # -------- CREMA-D --------
    elif dataset.lower() == 'cremad':
        if 'ActorID' in df.columns:
            df = df.rename(columns={'ActorID': 'actor_id'})
        else:
            # 文件名 1040_IEO_ANG_LO.wav → 1040
            df['actor_id'] = df['wav_file'].str.extract(r'^(\d{4})')
        return df
    # -------- RAVDESS --------
    elif dataset.lower() == 'ravdess':
        # 从文件夹 'Actor_XX/...' 或文件名 '..._XX.wav' 中提取
        df['actor_id'] = df['wav_file'].apply(
            lambda p: re.search(r'Actor[_-](\d{2})', p).group(1) if 'Actor' in p else p.split('/')[0][-2:]
            )
        df['actor_id'] = df['actor_id'].astype(int)
        return df

# ---------------------------------------------------------------------------
# Main splitting routine
# ---------------------------------------------------------------------------


def make_splits(iemocap_csv: Path, cremad_csv: Path, ravdess_csv: Path, out_dir: Path, seed: int = 0):
    out_dir.mkdir(parents=True, exist_ok=True)

    # ------------- load data
    df_iem = pd.read_csv(iemocap_csv)
    df_cre = pd.read_csv(cremad_csv)
    df_rav = pd.read_csv(ravdess_csv)

    # Ensure actor_id column exists
    df_iem = ensure_actor_id(df_iem, 'iemocap')
    df_cre = ensure_actor_id(df_cre, 'cremad')
    df_rav = ensure_actor_id(df_rav, 'ravdess')

    # Add corpus column (helpful downstream)
    df_iem["corpus"] = "iemocap"
    df_cre["corpus"] = "cremad"
    df_rav["corpus"] = "ravdess"

    for name, df in [('IEMOCAP', df_iem), ('CREMA-D', df_cre), ('RAVDESS', df_rav)]:
        print(name, 'unique speakers:', df['actor_id'].nunique())

    # Pre‑compute session column for IEMOCAP
    df_iem["session"] = df_iem["wav_file"].apply(_parse_iemocap_session)

    # Prepare RNGs
    base_rng = random.Random(seed)
    # np_base_rng = np.random.default_rng(seed)

    # iem_groups = [[s] for s in sorted(df_iem["session"].unique())]  # 5 sessions
    # base_rng.shuffle(iem_groups)  # shuffle sessions

    cre_groups = balanced_partition(df_cre["actor_id"].unique(), 5, base_rng)

    rav_groups = balanced_partition(df_rav["actor_id"].unique(), 5, base_rng)

    # 5 folds required (paper: create 5 different sets)
    for fold in range(5):
        rng = random.Random(seed + fold)        # deterministic per fold
        np_rng = np.random.default_rng(seed + fold)
        fold_dir = out_dir / f"fold_{fold}"
        fold_dir.mkdir(exist_ok=True)

        # ---- IEMOCAP sessions ----
        all_sessions = list(range(5))  # 0..4
        rng.shuffle(all_sessions)
        risk_sessions = sorted(all_sessions[:2])
        trans_sessions = sorted(all_sessions[2:4])
        test_sessions = sorted(all_sessions[4:])  # one session left

        iem_risk = df_iem[df_iem["session"].isin(risk_sessions)]
        iem_trans = df_iem[df_iem["session"].isin(trans_sessions)]
        iem_test = df_iem[df_iem["session"].isin(test_sessions)]

        # # ---- CREMA‑D actors ----
        # cre_actors_risk = set(_select_actor_ids(df_cre, 0.40, set(), rng))
        # cre_actors_trans = set(_select_actor_ids(df_cre, 0.40, cre_actors_risk, rng))
        # cre_actors_test = set(df_cre["actor_id"]) - cre_actors_risk - cre_actors_trans

        # cre_risk = df_cre[df_cre["actor_id"].isin(cre_actors_risk)]
        # cre_trans = df_cre[df_cre["actor_id"].isin(cre_actors_trans)]
        # cre_test = df_cre[df_cre["actor_id"].isin(cre_actors_test)]

        # # ---- RAVDESS actors ----
        # rav_actors_risk = set(_select_actor_ids(df_rav, 0.40, set(), rng))
        # rav_actors_trans = set(_select_actor_ids(df_rav, 0.40, rav_actors_risk, rng))
        # rav_actors_test = set(df_rav["actor_id"]) - rav_actors_risk - rav_actors_trans

        # rav_risk = df_rav[df_rav["actor_id"].isin(rav_actors_risk)]
        # rav_trans = df_rav[df_rav["actor_id"].isin(rav_actors_trans)]
        # rav_test = df_rav[df_rav["actor_id"].isin(rav_actors_test)]

        # # ---- concat per category ----
        # risk_full = pd.concat([iem_risk, cre_risk, rav_risk]).reset_index(drop=True)
        # trans_full = pd.concat([iem_trans, cre_trans, rav_trans]).reset_index(drop=True)
        # test_full = pd.concat([iem_test, cre_test, rav_test]).reset_index(drop=True)

        # role assignment for this fold
        # iem_test_s, iem_risk_s, iem_trans_s = create_fold_roles(iem_groups, fold)
        cre_test_a, cre_risk_a, cre_trans_a = create_fold_roles(cre_groups, fold)
        rav_test_a, rav_risk_a, rav_trans_a = create_fold_roles(rav_groups, fold)

        # slice dataframes
        

        cre_test  = df_cre[df_cre['actor_id'].isin(cre_test_a)]
        cre_risk  = df_cre[df_cre['actor_id'].isin(cre_risk_a)]
        cre_trans = df_cre[df_cre['actor_id'].isin(cre_trans_a)]

        rav_test  = df_rav[df_rav['actor_id'].isin(rav_test_a)]
        rav_risk  = df_rav[df_rav['actor_id'].isin(rav_risk_a)]
        rav_trans = df_rav[df_rav['actor_id'].isin(rav_trans_a)]

        # concat per role
        risk_full  = pd.concat([iem_risk,  cre_risk,  rav_risk ], ignore_index=True)
        trans_full = pd.concat([iem_trans, cre_trans, rav_trans], ignore_index=True)
        test_full  = pd.concat([iem_test,  cre_test,  rav_test ], ignore_index=True)

        # ---- train / val 75‑25 ----
        risk_train, risk_val = _train_val_split(risk_full, 0.75, np_rng)
        trans_train, trans_val = _train_val_split(trans_full, 0.75, np_rng)

        # ---- save ----
        risk_train.to_csv(fold_dir / "risk_train.csv", index=False)
        risk_val.to_csv(fold_dir / "risk_val.csv", index=False)
        trans_train.to_csv(fold_dir / "trans_train.csv", index=False)
        trans_val.to_csv(fold_dir / "trans_val.csv", index=False)
        test_full.to_csv(fold_dir / "test.csv", index=False)

        # ---- summary ----
        print(f"Fold {fold}")
        print(f"  IEM test sessions : {iem_test} | risk {iem_risk} | trans {iem_trans}")
        print(f"  CRE test actors   : {len(cre_test_a)} | risk {len(cre_risk_a)} | trans {len(cre_trans_a)}")
        print(f"  RAV test actors   : {len(rav_test_a)} | risk {len(rav_risk_a)} | trans {len(rav_trans_a)}")
        print(f"  Sizes -> risk_train {len(risk_train)} / risk_val {len(risk_val)} / "
              f"trans_train {len(trans_train)} / trans_val {len(trans_val)} / test {len(test_full)}\n")

# ---------------------------------------------------------------------------
# CLI
# ---------------------------------------------------------------------------


def main():
    p = argparse.ArgumentParser(description="Create χ^Risk, χ^R, χ^I splits for 5 folds.")
    p.add_argument("--iemocap_csv", type=Path, required=True)
    p.add_argument("--cremad_csv",   type=Path, required=True)
    p.add_argument("--ravdess_csv",  type=Path, required=True)
    p.add_argument("--out_dir",      type=Path, required=True)
    p.add_argument("--seed", type=int, default=42, help="Random seed")
    args = p.parse_args()

    make_splits(args.iemocap_csv, args.cremad_csv, args.ravdess_csv, args.out_dir, args.seed)


if __name__ == "__main__":
    main()
